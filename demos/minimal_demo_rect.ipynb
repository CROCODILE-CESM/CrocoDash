{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a regional MOM6 run within CESM framework (with CRR grid generation)\n",
    "\n",
    "There are three main sections: \n",
    "1. First we create the MOM6 experiment using the [regional-mom6](https://github.com/COSIMA/regional-mom6/) package. This puts everything we need into two repositories, referred to as the `mom_run_dir` and `mom_input_dir`. These contain all of the configuration files (`MOM_input` etc.) and netcdf input files (`hgrid.nc`, OBC segments etc.) respectively. An `experiment` object is also created within the notebook, containing all the information we need about the regional mom6 setup to pass onto CESM.\n",
    "\n",
    "2. A new MOM6 CESM case is created in the usual way by cloning the CESM repo and running the `create_case` command. \n",
    "\n",
    "3. We modify the new CESM case to work with the regional mom6 configuration we prepared earlier. We pass the path to this CESM run and the experiment object to the `setup_cesm` function, which carries out all the required modifications \n",
    "\n",
    "This is very much a work in progress! The goal in the short term was to get something working and make it nice later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: Setup up you MOM6 regional experiment\n",
    "\n",
    "This follows the normal workflow, copied from the `reanalysis_forced.ipynb` demo of [regional-mom6](https://github.com/COSIMA/regional-mom6/) but modified for a domain around Hawaii. See the documentation of the package for details and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, '/glade/u/home/manishrv/documents/nwa12_0.1/regional_mom_workflows/crr')\n",
    "import crocodileregionalruckus as crr\n",
    "from crocodileregionalruckus.rm6 import regional_mom6 as rmom6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose our domain, define workspace paths\n",
    "\n",
    "To make sure that things are working I'd recommend starting with the default example defined below. If this runs ok, then change to a domain of your choice and hopefully it runs ok too! If not, check the [README](https://github.com/COSIMA/regional-mom6/blob/main/README.md) and [documentation](https://regional-mom6.readthedocs.io/) for troubleshooting tips.\n",
    "\n",
    "You can log in and use [this GUI](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/download) to find the lat/lon of your domain and copy paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = \"crr-fresh-hawaii-t2\"\n",
    "\n",
    "latitude_extent = [16., 27]\n",
    "longitude_extent = [192, 209]\n",
    "\n",
    "date_range = [\"2020-01-01 00:00:00\", \"2020-02-01 00:00:00\"]\n",
    "\n",
    "## Place where all your input files go \n",
    "input_dir = Path(f\"/glade/u/home/manishrv/documents/nwa12_0.1/mom_input/{expt_name}/\")\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "run_dir = Path(f\"/glade/u/home/manishrv/documents/nwa12_0.1/mom_run/{expt_name}/\")\n",
    "\n",
    "## Directory where compiled FRE tools are located (needed for construction of mask tables)\n",
    "toolpath_dir = Path(\"\")\n",
    "\n",
    "## Path to where your raw ocean forcing files are stored\n",
    "glorys_path = os.path.join(\"/\",\"glade\",\"derecho\",\"scratch\",\"manishrv\",\"inputs_rm6_hawaii\",\"glorys\" )\n",
    "\n",
    "## if directories don't exist, create them\n",
    "for path in (run_dir, glorys_path, input_dir):\n",
    "    os.makedirs(str(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Grid Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid gen object\n",
    "grid_gen_obj = crr.grid_gen.GridGen() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hgrid\n",
    "resolution =0.05\n",
    "rect_hgrid = grid_gen_obj.create_rectangular_hgrid(latitude_extent = latitude_extent, longitude_extent = longitude_extent, resolution = resolution)\n",
    "#subset_global_hgrid = grid_gen_obj.subset_global_hgrid(longitude_extent, latitude_extent)\n",
    "\n",
    "# Create Vgrid\n",
    "gen_vgrid = grid_gen_obj.create_vgrid(75,10,depth = 4500, minimum_depth = 25)\n",
    "\n",
    "# Move desired hgrid, and vgrid to mom_input with the filenames that regional mom6 recognizes\n",
    "rect_hgrid.to_netcdf(input_dir / \"hgrid.nc\")\n",
    "gen_vgrid.to_netcdf(input_dir / \"vcoord.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make experiment object\n",
    "The `regional_mom6.experiment` contains the regional domain basics, and also generates the horizontal and vertical grids, `hgrid` and `vgrid` respectively, and sets up the directory structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = rmom6.experiment(\n",
    "    longitude_extent = longitude_extent,\n",
    "    latitude_extent = latitude_extent,\n",
    "    date_range = date_range,\n",
    "    resolution = 0.05,\n",
    "    number_vertical_layers = 75,\n",
    "    layer_thickness_ratio = 10,\n",
    "    depth = 4500,\n",
    "    minimum_depth = 25,\n",
    "    mom_run_dir = run_dir,\n",
    "    mom_input_dir = input_dir,\n",
    "    toolpath_dir = toolpath_dir,\n",
    "    hgrid_type = \"from_file\", # This is how we incorporate the grid_gen files\n",
    "    vgrid_type = \"from_file\",\n",
    "    expt_name = expt_name,\n",
    "        boundaries = [\"south\", \"north\", \"west\", \"east\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is `\"east_unprocessed\"` for segments and `\"ic_unprocessed\"` for the initial condition.\n",
    "\n",
    "In this notebook, we are forcing with the Copernicus Marine \"Glorys\" reanalysis dataset. There's a function in the `mom6-regional` package that generates a bash script to download the correct boundary forcing files for your experiment. First, you will need to create an account with Copernicus, and then call `copernicusmarine login` to set up your login details on your machine. Then you can run the `get_glorys_data.sh` bash script.\n",
    "\n",
    "The function is called `get_glorys_rectangular` because the fully automated setup is only supported for domains with boundaries parallel to lines of longitude and latitude. To download more complex domain shapes you can call `rmom6.get_glorys_data` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.get_glorys_rectangular(\n",
    "    raw_boundaries_path=glorys_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up bathymetry with grid gen\n",
    "\n",
    "Similarly to ocean forcing, we point the experiment's `setup_bathymetry` method at the location of the file of choice and also provide the variable names. We don't need to preprocess the bathymetry since it is simply a two-dimensional field and is easier to deal with. Afterwards you can inspect `expt.bathymetry` to have a look at the regional domain.\n",
    "\n",
    "After running this cell, your input directory will contain other bathymetry-related things like the ocean mosaic and mask table too. The mask table defaults to a 10x10 layout and can be modified later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin regridding bathymetry...\n",
      "\n",
      "Original bathymetry size: 99.59 Mb\n",
      "Regridded size: 1.93 Mb\n",
      "Automatic regridding may fail if your domain is too big! If this process hangs or crashes,open a terminal with appropriate computational and resources try calling ESMF directly in the input directory /glade/u/home/manishrv/documents/nwa12_0.1/mom_input/crr-fresh-hawaii-t2 via\n",
      "\n",
      "`mpirun -np NUMBER_OF_CPUS ESMF_Regrid -s bathymetry_original.nc -d bathymetry_unfinished.nc -m bilinear --src_var depth --dst_var depth --netcdf4 --src_regional --dst_regional`\n",
      "\n",
      "For details see https://xesmf.readthedocs.io/en/latest/large_problems_on_HPC.html\n",
      "\n",
      "Afterwards, run the 'expt.tidy_bathymetry' method to skip the expensive interpolation step, and finishing metadata, encoding and cleanup.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bathymetry_path='/glade/u/home/manishrv/manish_scratch_symlink/inputs_rm6/gebco/GEBCO_2024.nc'\n",
    "expt.bathymetry = grid_gen_obj.setup_bathymetry(input_dir = expt.mom_input_dir,\n",
    "                                                longitude_extent=longitude_extent, \n",
    "                                                latitude_extent = latitude_extent, \n",
    "                                                minimum_depth = expt.minimum_depth,  \n",
    "                                                bathymetry_path = bathymetry_path, \n",
    "                                                longitude_coordinate_name=\"lon\",\n",
    "                                                latitude_coordinate_name=\"lat\", \n",
    "                                                vertical_coordinate_name=\"elevation\", \n",
    "                                                hgrid = expt.hgrid)\n",
    "\n",
    "# OR, do it the normal way from the expirement object (which is how this wrapped currently)\n",
    "#expt.setup_bathymetry(....)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.tidy_bathymetry() # Looks for bathymetry file in the input directory, make sure you passed in the correct patyh in in setup_bathymetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "expt.bathymetry = xr.open_dataset(\"/glade/u/home/manishrv/documents/nwa12_0.1/mom_input/crr-fresh-hawaii-t2/bathymetry.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-ignore-output",
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "expt.bathymetry.depth.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then mask out parts of the domain we don't want by selecting a point in the ocean domain we do want.\n",
    "bathymetry = grid_gen_obj.mask_disconnected_ocean_areas(hgrid = expt.hgrid, \n",
    "                                                        topo = expt.bathymetry.depth, \n",
    "                                                        name_x_dim = \"x\", \n",
    "                                                        name_y_dim = \"y\", \n",
    "                                                        lat_pt = 20, \n",
    "                                                        lon_pt = 200)\n",
    "bathymetry.to_netcdf(expt.mom_input_dir / \"bathymetry.nc\")\n",
    "expt.bathymetry = bathymetry # Once we saved the bathymetry in the right place, we can set it in expt.bathymetry just to be sure. \n",
    "expt.bathymetry.depth.plot()\n",
    "expt.bathymetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Handle the ocean forcing - where the magic happens\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the MOM6 variable names to what they're called in your ocean input file. Notice how for GLORYS, the horizontal dimensions are `latitude` and `longitude`, vs `xh`, `yh`, `xq`, `yq` for MOM6. This is because for an 'A' grid type tracers share the grid with velocities so there's no difference.\n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL CONDITIONS\n",
      "Regridding Velocities... Done.\n",
      "Regridding Tracers... Done.\n",
      "Regridding Free surface... Done.\n",
      "Saving outputs... done setting up initial condition.\n",
      "[########################################] | 100% Completed | 102.10 ms\n",
      "Done.\n",
      "[########################################] | 100% Completed | 103.20 ms\n",
      "Done.\n",
      "[########################################] | 100% Completed | 102.12 ms\n",
      "Done.\n",
      "[########################################] | 100% Completed | 101.98 ms\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping from the GLORYS variables and dimensions to the MOM6 ones\n",
    "ocean_varnames = {\"time\": \"time\",\n",
    "                  \"yh\": \"latitude\",\n",
    "                  \"xh\": \"longitude\",\n",
    "                  \"zl\": \"depth\",\n",
    "                  \"eta\": \"zos\",\n",
    "                  \"u\": \"uo\",\n",
    "                  \"v\": \"vo\",\n",
    "                  \"tracers\": {\"salt\": \"so\", \"temp\": \"thetao\"}\n",
    "                  }\n",
    "\n",
    "# Set up the initial condition\n",
    "expt.setup_initial_condition(\n",
    "    Path(glorys_path) / \"ic_unprocessed.nc\", # directory where the unprocessed initial condition is stored, as defined earlier\n",
    "    ocean_varnames,\n",
    "    arakawa_grid=\"A\"\n",
    "    )    \n",
    "\n",
    "# Set up the four boundary conditions. Remember that in the glorys_path, we have four boundary files names north_unprocessed.nc etc. \n",
    "expt.setup_ocean_state_boundaries(\n",
    "        Path(glorys_path),\n",
    "        ocean_varnames,\n",
    "        arakawa_grid = \"A\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing north boundary...Done\n",
      "Processing south boundary...Done\n",
      "Processing east boundary...Done\n",
      "Processing west boundary...Done\n"
     ]
    }
   ],
   "source": [
    "expt.setup_boundary_tides(Path(\"/glade/u/home/manishrv/manish_scratch_symlink/inputs_rm6/tidal_data\"),Path(\"tpxo9.v1.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This step copies the default directory and modifies the `MOM_layout` files to match your experiment by inserting the right number of x, y points and CPU layout.\n",
    "\n",
    "To run MOM6 using the [payu infrastructure](https://github.com/payu-org/payu), provide the keyword argument `using_payu = True` to the `setup_run_directory` method and an example `config.yaml` file will be appear in the run directory. The `config.yaml` file needs to be modified manually to add the locations of executables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find premade run directories at  /glade/u/home/manishrv/documents/nwa12_0.1/regional_mom_workflows/crr/crocodileregionalruckus/rm6/regional_mom6/demos/premade_run_directories\n",
      "Perhaps the package was imported directly rather than installed with conda. Checking if this is the case... \n",
      "Found run files. Continuing...\n",
      "WARNING: No mask table found, and the cpu layout has not been set. \n",
      "At least one of these is requiret to set up the experiment if you're running MOM6 standalone with the FMS coupler. \n",
      "If you're running within CESM, ignore this message.\n",
      "Changed NIGLOBAL from {'value': '3040', 'override': False, 'comment': '\\n'} to {'value': 340, 'override': False, 'comment': '\\n'} in MOM_layout!\n",
      "Changed NJGLOBAL from {'value': '3180', 'override': False, 'comment': '\\n'} to {'value': 236, 'override': False, 'comment': '\\n'} in MOM_layout!\n",
      "Changed MASKTABLE from {'value': '\"mask_table.766.80x80\"', 'override': False, 'comment': ' default = \"MOM_mask_table\"\\n'} to {'value': '# MASKTABLE = no mask table', 'override': False, 'comment': ' default = \"MOM_mask_table\"\\n'} in MOM_layout!\n",
      "Deleting indexed OBC keys from MOM_input_dict in case we have a different number of segments\n",
      "Added MINIMUM_DEPTH to MOM_override with value {'value': 25.0, 'override': True, 'comment': None}\n",
      "Added NK to MOM_override with value {'value': 75, 'override': True, 'comment': None}\n",
      "Added OBC_NUMBER_OF_SEGMENTS to MOM_override with value {'value': 44, 'override': True, 'comment': None}\n",
      "Added OBC_FREESLIP_VORTICITY to MOM_override with value {'value': 'False', 'override': True, 'comment': None}\n",
      "Added OBC_FREESLIP_STRAIN to MOM_override with value {'value': 'False', 'override': True, 'comment': None}\n",
      "Added OBC_COMPUTED_VORTICITY to MOM_override with value {'value': 'True', 'override': True, 'comment': None}\n",
      "Added OBC_COMPUTED_STRAIN to MOM_override with value {'value': 'True', 'override': True, 'comment': None}\n",
      "Added OBC_ZERO_BIHARMONIC to MOM_override with value {'value': 'True', 'override': True, 'comment': None}\n",
      "Added OBC_TRACER_RESERVOIR_LENGTH_SCALE_OUT to MOM_override with value {'value': '3.0E+04', 'override': True, 'comment': None}\n",
      "Added OBC_TRACER_RESERVOIR_LENGTH_SCALE_IN to MOM_override with value {'value': '3000.0', 'override': True, 'comment': None}\n",
      "Added BRUSHCUTTER_MODE to MOM_override with value {'value': 'True', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_001 to MOM_override with value {'value': '\"J=N,I=N:0,FLATHER,ORLANSKI,NUDGED,ORLANSKI_TAN,NUDGED_TAN\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_001_VELOCITY_NUDGING_TIMESCALES to MOM_override with value {'value': '0.3, 360.0', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_001_DATA to MOM_override with value {'value': '\"U=file:forcing_obc_segment_001.nc(u),V=file:forcing_obc_segment_001.nc(v),SSH=file:forcing_obc_segment_001.nc(eta),TEMP=file:forcing_obc_segment_001.nc(temp),SALT=file:forcing_obc_segment_001.nc(salt)\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_002 to MOM_override with value {'value': '\"J=0,I=0:N,FLATHER,ORLANSKI,NUDGED,ORLANSKI_TAN,NUDGED_TAN\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_002_VELOCITY_NUDGING_TIMESCALES to MOM_override with value {'value': '0.3, 360.0', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_002_DATA to MOM_override with value {'value': '\"U=file:forcing_obc_segment_002.nc(u),V=file:forcing_obc_segment_002.nc(v),SSH=file:forcing_obc_segment_002.nc(eta),TEMP=file:forcing_obc_segment_002.nc(temp),SALT=file:forcing_obc_segment_002.nc(salt)\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_003 to MOM_override with value {'value': '\"I=N,J=0:N,FLATHER,ORLANSKI,NUDGED,ORLANSKI_TAN,NUDGED_TAN\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_003_VELOCITY_NUDGING_TIMESCALES to MOM_override with value {'value': '0.3, 360.0', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_003_DATA to MOM_override with value {'value': '\"U=file:forcing_obc_segment_003.nc(u),V=file:forcing_obc_segment_003.nc(v),SSH=file:forcing_obc_segment_003.nc(eta),TEMP=file:forcing_obc_segment_003.nc(temp),SALT=file:forcing_obc_segment_003.nc(salt)\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_004 to MOM_override with value {'value': '\"I=0,J=N:0,FLATHER,ORLANSKI,NUDGED,ORLANSKI_TAN,NUDGED_TAN\"', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_004_VELOCITY_NUDGING_TIMESCALES to MOM_override with value {'value': '0.3, 360.0', 'override': True, 'comment': None}\n",
      "Added OBC_SEGMENT_004_DATA to MOM_override with value {'value': '\"U=file:forcing_obc_segment_004.nc(u),V=file:forcing_obc_segment_004.nc(v),SSH=file:forcing_obc_segment_004.nc(eta),TEMP=file:forcing_obc_segment_004.nc(temp),SALT=file:forcing_obc_segment_004.nc(salt)\"', 'override': True, 'comment': None}\n"
     ]
    }
   ],
   "source": [
    "expt.setup_run_directory(surface_forcing = \"jra\", with_tides = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: Create a blank CESM run\n",
    "\n",
    "So far I've used Alper's [GUI](https://github.com/ESMCI/visualCaseGen?tab=readme-ov-file) branch of CESM. Clone respective branch of CESM and then run the generate case command. Below is the command I used to generate a global, MOM6 only run forced with JRA data atmosphere\n",
    "\n",
    "`/glade/u/home/abarnes/cesm-runs/visualCaseGen/cesm2_3_beta17_gui/cime/scripts/create_newcase --compset 1850_DATM%JRA_SLND_SICE_MOM6_SROF_SGLC_SWAV --res TL319_t232 --case /glade/u/home/manishrv/cases/crr-fresh-hawaii-t2 --machine derecho --run-unsupported --project p93300012 --non-local`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the path where the new CESM config lives\n",
    "CESM_path = Path(f\"/glade/u/home/manishrv/cases/crr-fresh-hawaii-t2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Modify the CESM run to make it regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regional CaseGen is part of crr\n",
    "\n",
    "reg_cas_obj = crr.regional_casegen.cesm_tools.RegionalCaseGen()\n",
    "\n",
    "\n",
    "reg_cas_obj.setup_cesm(expt, CESM_path, project = \"p93300612\")\n",
    "## OR you can explicitly state what variables are needed\n",
    "# reg_cas_obj.setup_cesm_explicit(hgrid = expt.hgrid, CESMPath = CESM_path,project = \"p93300612\", mom_input_dir=expt.mom_input_dir, mom_run_dir=expt.mom_run_dir,date_range = expt.date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now hopefully doing the usual \n",
    "\n",
    "`./case.setup && ./case.build && ./case.submit` \n",
    "\n",
    "should at least run. Of course from here you'll have a lot of other things to fiddle around with to make it run *well!* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
